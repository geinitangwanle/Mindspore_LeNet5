{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mindspore\n",
    "from mindspore import nn\n",
    "from mindspore import context, dataset\n",
    "from mindspore.dataset import vision, transforms\n",
    "from mindspore import save_checkpoint, load_checkpoint, load_param_into_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(51660:8531184768,MainProcess):2025-05-03-11:28:37.320.049 [mindspore/context.py:1335] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子以确保结果可复现\n",
    "np.random.seed(42)\n",
    "mindspore.set_seed(42)\n",
    "\n",
    "# 设置matplotlib支持中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['PingFang SC', 'Heiti TC', 'SimHei', 'Arial']\n",
    "\n",
    "# 设置运行模式为图模式，提高性能\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_names():\n",
    "    \"\"\"获取数据集的类别名称\"\"\"\n",
    "    class_names = os.listdir('data_en/train')\n",
    "    class_names.sort()  # 确保类别顺序一致\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    \"\"\"准备垃圾分类数据集，返回处理好的训练集和测试集\"\"\"\n",
    "    # 获取类别名称\n",
    "    class_names = get_class_names()\n",
    "    num_classes = len(class_names)\n",
    "    print(f\"数据集包含 {num_classes} 个类别: {class_names}\")\n",
    "    \n",
    "    # 创建类别到索引的映射\n",
    "    class_to_idx = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "    \n",
    "    # 使用ImageFolderDataset加载数据\n",
    "    train_dataset = dataset.ImageFolderDataset('data_en/train', class_indexing=class_to_idx, \n",
    "                                              shuffle=True, decode=True)\n",
    "    test_dataset = dataset.ImageFolderDataset('data_en/test', class_indexing=class_to_idx, \n",
    "                                             shuffle=False, decode=True)\n",
    "    \n",
    "    print(f\"训练集大小: {train_dataset.get_dataset_size()}\")\n",
    "    print(f\"测试集大小: {test_dataset.get_dataset_size()}\")\n",
    "    \n",
    "    # 数据增强和预处理\n",
    "    def datapipe(ds, batch_size, is_training=True):\n",
    "        # 针对训练集的数据增强\n",
    "        if is_training:\n",
    "            image_transforms = [\n",
    "                vision.Resize((96, 96)),                          # 调整大小到96x96\n",
    "                vision.RandomCrop((88, 88)),                      # 随机裁剪到88x88以保留更多细节\n",
    "                vision.RandomHorizontalFlip(prob=0.5),            # 随机水平翻转\n",
    "                vision.RandomColorAdjust(brightness=0.1, contrast=0.1),  # 轻微颜色调整\n",
    "                vision.Rescale(1.0 / 255.0, 0),                   # 将像素值缩放到[0, 1]范围\n",
    "                vision.Normalize(mean=[0.485, 0.456, 0.406],      # 标准化处理\n",
    "                               std=[0.229, 0.224, 0.225]),\n",
    "                vision.HWC2CHW()                                  # 转换通道顺序\n",
    "            ]\n",
    "        else:\n",
    "            # 测试集只需要调整大小和标准化\n",
    "            image_transforms = [\n",
    "                vision.Resize((96, 96)),                          # 调整大小\n",
    "                vision.CenterCrop((88, 88)),                      # 中心裁剪\n",
    "                vision.Rescale(1.0 / 255.0, 0),\n",
    "                vision.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225]),\n",
    "                vision.HWC2CHW()\n",
    "            ]\n",
    "            \n",
    "        # 标签转换为int32类型\n",
    "        label_transform = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "        # 应用数据转换\n",
    "        ds = ds.map(image_transforms, 'image')\n",
    "        ds = ds.map(label_transform, 'label')\n",
    "        # 设置批处理大小\n",
    "        ds = ds.batch(batch_size, drop_remainder=False)\n",
    "        \n",
    "        # 对训练数据进行混洗\n",
    "        if is_training:\n",
    "            ds = ds.shuffle(buffer_size=batch_size * 10)\n",
    "            \n",
    "        return ds\n",
    "\n",
    "    # 设置批处理大小\n",
    "    batch_size = 32\n",
    "    train_dataset = datapipe(train_dataset, batch_size, is_training=True)\n",
    "    test_dataset = datapipe(test_dataset, batch_size, is_training=False)\n",
    "    \n",
    "    # 查看处理后的数据格式\n",
    "    for image, label in test_dataset.create_tuple_iterator():\n",
    "        print(f\"图像形状 [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "        print(f\"标签形状: {label.shape} {label.dtype}\")\n",
    "        break\n",
    "        \n",
    "    return train_dataset, test_dataset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedLeNet5(nn.Cell):\n",
    "    \"\"\"平衡的LeNet5模型，适度增加深度和宽度\"\"\"\n",
    "    def __init__(self, num_classes=26):\n",
    "        super(BalancedLeNet5, self).__init__()\n",
    "        \n",
    "        # 第一个卷积块 - 增加通道数\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, pad_mode='same')\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 第二个卷积块\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, pad_mode='same')\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 第三个卷积块\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, pad_mode='same')\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 第四个卷积块\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, pad_mode='same')\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 展平层\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # 计算展平后的特征尺寸\n",
    "        # 输入88x88经过4次池化后变为5x5\n",
    "        flattened_size = 5 * 5 * 128\n",
    "        \n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Dense(flattened_size, 256)\n",
    "        self.fc_relu1 = nn.ReLU()\n",
    "        self.fc_dropout1 = nn.Dropout(0.3)  # 降低dropout率\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc2 = nn.Dense(256, num_classes)\n",
    "        \n",
    "    def construct(self, x):\n",
    "        # 第一个卷积块\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # 第二个卷积块\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # 第三个卷积块\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # 第四个卷积块\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        # 展平\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc_relu1(x)\n",
    "        x = self.fc_dropout1(x)\n",
    "        \n",
    "        # 输出层\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, test_dataset, epochs=25):\n",
    "    \"\"\"训练和评估模型\"\"\"\n",
    "    # 定义损失函数\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 学习率预热和阶梯式衰减\n",
    "    lr_init = 0.0001  # 较低的初始学习率\n",
    "    lr_max = 0.001    # 预热后的最大学习率\n",
    "    warmup_epochs = 3  # 预热阶段的epoch数\n",
    "    \n",
    "    # 分段学习率调度，先预热再阶梯式衰减\n",
    "    '''先线性预热，再阶梯式衰减'''\n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            # 线性预热\n",
    "            return lr_init + (lr_max - lr_init) * epoch / warmup_epochs\n",
    "        elif epoch < 15:\n",
    "            return lr_max\n",
    "        elif epoch < 20:\n",
    "            return lr_max * 0.1\n",
    "        else:\n",
    "            return lr_max * 0.01\n",
    "    \n",
    "    # 定义优化器 - 使用Adam优化器，较小的权重衰减\n",
    "    optimizer = nn.Adam(\n",
    "        params=model.trainable_params(),\n",
    "        learning_rate=lr_init,  # 初始使用较小的学习率\n",
    "        weight_decay=1e-5       # 降低权重衰减\n",
    "    )\n",
    "    \n",
    "    # 定义前向计算和梯度计算函数\n",
    "    def forward_fn(data, label):\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, label)\n",
    "        return loss, output\n",
    "\n",
    "    grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "    \n",
    "    # 定义训练步骤\n",
    "    def train_step(data, label, lr):\n",
    "        # 更新学习率\n",
    "        for param in optimizer.parameters:\n",
    "            if param.name == 'learning_rate':\n",
    "                param.set_data(mindspore.Tensor(lr, mindspore.float32))\n",
    "        \n",
    "        (loss, output), grads = grad_fn(data, label)\n",
    "        optimizer(grads)\n",
    "        pred = output.argmax(1)\n",
    "        acc = (pred == label).asnumpy().mean()\n",
    "        return loss, acc\n",
    "    \n",
    "    # 用于早停的变量\n",
    "    best_acc = 0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # 训练循环\n",
    "    print(\"开始训练平衡版LeNet5模型...\")\n",
    "    for epoch in range(epochs):\n",
    "        # 设置当前学习率\n",
    "        current_lr = lr_scheduler(epoch)\n",
    "        print(f\"Epoch {epoch+1} - 学习率: {current_lr:.6f}\")\n",
    "        \n",
    "        # 训练阶段\n",
    "        model.set_train(True)\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        for data, label in train_dataset.create_tuple_iterator():\n",
    "            loss, acc = train_step(data, label, current_lr)\n",
    "            train_loss += loss.asnumpy()\n",
    "            train_acc += acc\n",
    "            train_steps += 1\n",
    "            \n",
    "            if train_steps % 20 == 0:\n",
    "                print(f\"  Batch {train_steps}: loss={loss.asnumpy():.4f}, acc={acc:.4f}\")\n",
    "            \n",
    "        # 计算平均训练损失和准确率\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "        avg_train_acc = train_acc / train_steps\n",
    "        \n",
    "        # 评估阶段\n",
    "        model.set_train(False)\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # 创建混淆矩阵\n",
    "        num_classes = test_dataset.output_shapes()[1][0]  # 获取类别数\n",
    "        confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
    "        \n",
    "        for data, label in test_dataset.create_tuple_iterator():\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, label)\n",
    "            test_loss += loss.asnumpy()\n",
    "            \n",
    "            pred = output.argmax(1)\n",
    "            correct += (pred == label).asnumpy().sum()\n",
    "            total += label.shape[0]\n",
    "            \n",
    "            # 更新混淆矩阵\n",
    "            for i in range(len(label)):\n",
    "                confusion_matrix[label[i], pred[i]] += 1\n",
    "        \n",
    "        avg_test_loss = test_loss / test_dataset.get_dataset_size()\n",
    "        test_acc = correct / total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  训练损失: {avg_train_loss:.4f}, 训练准确率: {avg_train_acc:.4f}\")\n",
    "        print(f\"  测试损失: {avg_test_loss:.4f}, 测试准确率: {test_acc:.4f}\")\n",
    "        \n",
    "        # 计算每个类别的准确率\n",
    "        per_class_acc = np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n",
    "        # 筛选出表现最差的5个类别\n",
    "        worst_classes = np.argsort(per_class_acc)[:5]\n",
    "        print(\"  表现最差的5个类别:\")\n",
    "        class_names = get_class_names()\n",
    "        for idx in worst_classes:\n",
    "            if np.sum(confusion_matrix[idx]) > 0:  # 确保该类有样本\n",
    "                acc = per_class_acc[idx]\n",
    "                print(f\"    {class_names[idx]}: {acc:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            save_checkpoint(model, f\"balanced_lenet5_epoch_{epoch+1}.ckpt\")\n",
    "            print(f\"  新的最佳模型已保存! 准确率: {best_acc:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # 每5个epoch保存一次模型，便于后续分析\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_checkpoint(model, f\"balanced_lenet5_epoch_{epoch+1}_checkpoint.ckpt\")\n",
    "            \n",
    "        # 早停检查\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"早停：连续{patience}个epoch没有改进，停止训练\")\n",
    "            break\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, test_dataset, class_names):\n",
    "    \"\"\"可视化模型预测结果\"\"\"\n",
    "    model.set_train(False)\n",
    "    \n",
    "    # 获取一批测试数据\n",
    "    for images, labels in test_dataset.create_tuple_iterator():\n",
    "        break\n",
    "        \n",
    "    # 获取预测结果\n",
    "    output = model(images)\n",
    "    predictions = output.argmax(1).asnumpy()\n",
    "    labels = labels.asnumpy()\n",
    "    images = images.asnumpy()\n",
    "    \n",
    "    # 计算每个类别的准确率\n",
    "    class_correct = {}\n",
    "    class_total = {}\n",
    "    \n",
    "    for image, label, pred in zip(images, labels, predictions):\n",
    "        class_name = class_names[label]\n",
    "        if class_name not in class_total:\n",
    "            class_total[class_name] = 0\n",
    "            class_correct[class_name] = 0\n",
    "            \n",
    "        class_total[class_name] += 1\n",
    "        if label == pred:\n",
    "            class_correct[class_name] += 1\n",
    "    \n",
    "    # 打印每个类别的准确率\n",
    "    print(\"\\n每个类别的准确率:\")\n",
    "    for class_name in class_names:\n",
    "        if class_name in class_total and class_total[class_name] > 0:\n",
    "            accuracy = class_correct.get(class_name, 0) / class_total[class_name]\n",
    "            print(f\"{class_name}: {accuracy:.2f} ({class_correct.get(class_name, 0)}/{class_total[class_name]})\")\n",
    "    \n",
    "    # 绘制图像和预测结果\n",
    "    # 图像反归一化函数\n",
    "    def denormalize(image):\n",
    "        image = image.transpose(1, 2, 0)  # CHW to HWC\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = image * std + mean  # 反归一化\n",
    "        return np.clip(image, 0, 1)\n",
    "    \n",
    "    # 显示图像\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(min(25, len(images))):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        \n",
    "        # 显示图像\n",
    "        plt.imshow(denormalize(images[i]))\n",
    "        \n",
    "        # 预测正确显示绿色，错误显示红色\n",
    "        if predictions[i] == labels[i]:\n",
    "            color = 'green'\n",
    "            plt.xlabel(f\"{class_names[predictions[i]]}\", color=color)\n",
    "        else:\n",
    "            color = 'red'\n",
    "            plt.xlabel(f\"预: {class_names[predictions[i]]}\\n实: {class_names[labels[i]]}\", color=color)\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./balanced_lenet5_predictions.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
